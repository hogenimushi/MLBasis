{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainer チュートリアル その4\n",
    "\n",
    "https://qiita.com/mitmul/items/eccf4e0a84cb784ba84a\n",
    "のなかから、Chainarでデータセットクラスを作成してみる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  前回の復習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer.datasets import cifar\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "\n",
    "def train(model_object, batchsize=64, gpu_id=0, max_epoch=20, train_dataset=None, test_dataset=None):\n",
    "\n",
    "    # 1. Dataset\n",
    "    if train_dataset is None and test_dataset is None:\n",
    "        train, test = cifar.get_cifar10()\n",
    "    else:\n",
    "        train, test = train_dataset, test_dataset\n",
    "\n",
    "    # 2. Iterator\n",
    "    train_iter = iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = iterators.SerialIterator(test, batchsize, False, False)\n",
    "\n",
    "    # 3. Model\n",
    "    model = L.Classifier(model_object)\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "\n",
    "    # 4. Optimizer\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # 5. Updater\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "\n",
    "    # 6. Trainer\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='{}_cifar10_result'.format(model_object.__class__.__name__))\n",
    "\n",
    "    # 7. Evaluator\n",
    "\n",
    "    class TestModeEvaluator(extensions.Evaluator):\n",
    "\n",
    "        def evaluate(self):\n",
    "            model = self.get_target('main')\n",
    "            model.train = False\n",
    "            ret = super(TestModeEvaluator, self).evaluate()\n",
    "            model.train = True\n",
    "            return ret\n",
    "\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(TestModeEvaluator(test_iter, model, device=gpu_id))\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    trainer.run()\n",
    "    del trainer\n",
    "\n",
    "    return model\n",
    "\n",
    "class ConvBlock(chainer.Chain):\n",
    "    def __init__(self, n_ch, pool_drop=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ConvBlock, self).__init__(\n",
    "            conv=L.Convolution2D(None, n_ch, 3, 1, 1,\n",
    "                                 nobias=True, initialW=w),\n",
    "            bn=L.BatchNormalization(n_ch)\n",
    "        )\n",
    "\n",
    "        self.train = True\n",
    "        self.pool_drop = pool_drop\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn(self.conv(x)))\n",
    "        if self.pool_drop:\n",
    "            h = F.max_pooling_2d(h, 2, 2)\n",
    "            h = F.dropout(h, ratio=0.25)\n",
    "        return h\n",
    "\n",
    "class LinearBlock(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(LinearBlock, self).__init__(\n",
    "            fc=L.Linear(None, 1024, initialW=w))\n",
    "        self.train = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return F.dropout(F.relu(self.fc(x)), ratio=0.5)\n",
    "\n",
    "class DeepCNN(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        super(DeepCNN, self).__init__(\n",
    "            ConvBlock(64),\n",
    "            ConvBlock(64, True),\n",
    "            ConvBlock(128),\n",
    "            ConvBlock(128, True),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256, True),\n",
    "            LinearBlock(),\n",
    "            LinearBlock(),\n",
    "            L.Linear(None, n_output)\n",
    "        )\n",
    "        self._train = True\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "\n",
    "    @train.setter\n",
    "    def train(self, val):\n",
    "        self._train = val\n",
    "        for c in self.children():\n",
    "            c.train = val\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットクラスの作成\n",
    "\n",
    "Deep leariningを行う際、データセットに何かしらの変更を加えて、学習成果を上げることはよく行われる。\n",
    "これをData augmentationと呼ぶ。Chainerでは、データセットクラスにData augmentationの内容を記述する。\n",
    "\n",
    "それでは、CIFAR10のデータを使って、データセットクラスを自分で作成してみよう。\n",
    "\n",
    "Chainerでは、データセットを表すクラスは以下の機能を実装する。\n",
    "\n",
    "- データセット内のデータ数を返す__len__メソッド\n",
    "- 引数として渡されるiに対応したデータもしくはデータとラベルの組を返すget_exampleメソッド\n",
    "\n",
    "その他の必要な機能は、chainer.dataset.DatasetMixinクラスを継承することで用意できる。\n",
    "Data augmentation機能のついたデータセットクラスを作成する際には、DatasetMixinクラスを継承しよう。\n",
    "\n",
    "これから作成するクラスは、CIFAR10のデータのそれぞれに対し、\n",
    "\n",
    "- 32x32の大きさの中からランダムに28x28の領域をクロップ\n",
    "- 1/2の確率で左右を反転させる\n",
    "\n",
    "という加工を行なう。\n",
    "\n",
    "これらの操作を行ない、学習データのバリエーションを増やすと、過学習抑制できることが知られている。\n",
    "他には、画像の色味を変化させるような変換、ランダムな回転、アフィン変換、などの加工によって学習データ数を擬似的に増やす方法が提案されているので、それらを使うとより効果的な学習を行うことができるだろう。\n",
    "\n",
    "自分でデータの取得する場合は、コンストラクタに画像フォルダのパスとファイル名に対応したラベルの書かれたテキストファイルへのパスなどを渡してプロパティとして保持しておき、get_exampleメソッド内でそれぞれの画像を読み込んで対応するラベルとともに返す、という処理を記述する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import dataset\n",
    "from chainer.datasets import cifar\n",
    "\n",
    "class CIFAR10Augmented(dataset.DatasetMixin):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        train_data, test_data = cifar.get_cifar10()\n",
    "        if train:\n",
    "            self.data = train_data\n",
    "        else:\n",
    "            self.data = test_data\n",
    "        self.train = train\n",
    "        self.random_crop = 4 #端から4ピクセル削る\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        x, t = self.data[i]\n",
    "        if self.train: # trainingの時は・・・\n",
    "            x = x.transpose(1, 2, 0)  # 2次元にする\n",
    "            h, w, _ = x.shape\n",
    "            x_offset = np.random.randint(self.random_crop) # 中心からのずれを\n",
    "            y_offset = np.random.randint(self.random_crop) # 乱数で決める\n",
    "            x = x[y_offset:y_offset + h - self.random_crop,\n",
    "                  x_offset:x_offset + w - self.random_crop] #領域をクロップする\n",
    "            if np.random.rand() > 0.5:\n",
    "                x = np.fliplr(x) # 確率1/2で左右反転\n",
    "            x = x.transpose(2, 0, 1) #1次元に戻す\n",
    "        return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 作成したデータセットクラスで学習を行う\n",
    "\n",
    "CIFAR10クラスを使って学習を行ってみましょう。先程使ったのと同じ大きなネットワークを使うことで、Data augmentationの効果がどの程度あるのかを調べてみましょう。train関数も含め、データセットクラス以外は先程使用したコードとほぼ同じになっています。異なるところはエポック数と、保存先ディレクトリ名だけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train(DeepCNN(10), max_epoch=100, train_dataset=CIFAR10Augmented(), test_dataset=CIFAR10Augmented(False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
